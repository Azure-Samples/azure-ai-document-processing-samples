{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../modules/') # Import local modules\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import dotenv_values\n",
    "import base64\n",
    "import io\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeResult, DocumentContentFormat\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pdf2image import convert_from_bytes\n",
    "\n",
    "from samples.app_settings import AppSettings\n",
    "from samples.utils.stopwatch import Stopwatch\n",
    "from samples.utils.storage_utils import create_json_file\n",
    "from samples.models.document_processing_result import DataExtractionResult\n",
    "\n",
    "from invoice import Invoice\n",
    "from classification import Classifications\n",
    "from samples.confidence.confidence_utils import merge_confidence_values\n",
    "from samples.confidence.openai_confidence import evaluate_confidence as evaluate_openai_confidence\n",
    "from samples.confidence.document_intelligence_confidence import evaluate_confidence as evaluate_di_confidence\n",
    "from samples.evaluation.accuracy_evaluator import AccuracyEvaluator\n",
    "from samples.evaluation.comparison import get_extraction_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory to the root of the repo\n",
    "working_dir = os.path.abspath('../../../../')\n",
    "settings = AppSettings(dotenv_values(f\"{working_dir}/.env\"))\n",
    "sample_path = f\"{working_dir}/samples/python/demo/invoices/\"\n",
    "sample_name = \"invoice-extraction\"\n",
    "\n",
    "# Configure the default credential for accessing Azure services using Azure CLI credentials\n",
    "credential = DefaultAzureCredential(\n",
    "    exclude_workload_identity_credential=True,\n",
    "    exclude_developer_cli_credential=True,\n",
    "    exclude_environment_credential=True,\n",
    "    exclude_managed_identity_credential=True,\n",
    "    exclude_powershell_credential=True,\n",
    "    exclude_shared_token_cache_credential=True,\n",
    "    exclude_interactive_browser_credential=True\n",
    ")\n",
    "\n",
    "openai_token_provider = get_bearer_token_provider(credential, 'https://cognitiveservices.azure.com/.default')\n",
    "\n",
    "openai_client = AzureOpenAI(\n",
    "    azure_endpoint=settings.openai_endpoint,\n",
    "    azure_ad_token_provider=openai_token_provider,\n",
    "    api_version=\"2024-12-01-preview\" # Requires the latest API version for structured outputs.\n",
    ")\n",
    "\n",
    "document_intelligence_client = DocumentIntelligenceClient(\n",
    "    endpoint=settings.ai_services_endpoint,\n",
    "    credential=credential\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = sample_path\n",
    "metadata_fname = \"invoice.json\" # Change this to the file you want to evaluate\n",
    "metadata_fpath = f\"{path}{metadata_fname}\"\n",
    "\n",
    "with open(metadata_fpath, 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "expected_outputs = [Invoice(**metadata[key]) for key in metadata.keys() if key.endswith('_expected')]\n",
    "\n",
    "pdf_fname = metadata['fname']\n",
    "pdf_fpath = f\"{path}{pdf_fname}\"\n",
    "\n",
    "use_document_intelligence = True\n",
    "\n",
    "invoice_evaluator = AccuracyEvaluator(match_keys=['product_code', 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_images = []\n",
    "\n",
    "def encode_page(page):\n",
    "    byte_io = io.BytesIO()\n",
    "    page.save(byte_io, format='PNG')\n",
    "    base64_data = base64.b64encode(byte_io.getvalue()).decode('utf-8')\n",
    "    return {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            \"url\": f\"data:image/png;base64,{base64_data}\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "with Stopwatch() as image_stopwatch:\n",
    "    with open(pdf_fpath, \"rb\") as f:\n",
    "        document_bytes = f.read()\n",
    "\n",
    "    pages = convert_from_bytes(document_bytes)\n",
    "    \n",
    "    # Process each page in parallel using multiple processes\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(encode_page, pages))\n",
    "        page_images.extend(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = [\n",
    "    {\n",
    "        \"classification\": \"Invoice/Credit Note\",\n",
    "        \"description\": \"A document that serves as a bill for goods or services provided, often used for payment processing and record-keeping.\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_system_prompt = f\"\"\"You are an AI assistant that helps detect the boundaries of sub-section or sub-documents using the provided classifications.\n",
    "\n",
    "## Classifications\n",
    "\n",
    "{json.dumps(classifications)}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_user_content = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_user_prompt = f\"\"\"Classify documents that are an invoice/credit note in the provided page images.\n",
    "- A single classification may span multiple page images.\n",
    "- A single page image may contain multiple classifications.\n",
    "- If a page image does not contain a classification, ignore it.\"\"\"\n",
    "\n",
    "classify_user_content.append({\n",
    "    \"type\": \"text\",\n",
    "    \"text\": classify_user_prompt\n",
    "})\n",
    "\n",
    "# for each of the page images, add a text content block with the page index, and then another block with the image\n",
    "for i, page_image in enumerate(page_images):\n",
    "    classify_user_content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": f\"Page {i + 1}:\"\n",
    "    })\n",
    "    classify_user_content.append(page_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Stopwatch() as classify_stopwatch:\n",
    "    classify_completion = openai_client.beta.chat.completions.parse(\n",
    "        model=settings.gpt4o_model_deployment_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": classify_system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": classify_user_content\n",
    "            }\n",
    "        ],\n",
    "        response_format=Classifications,\n",
    "        max_tokens=4096,\n",
    "        temperature=0.1,\n",
    "        top_p=0.1,\n",
    "        logprobs=True # Enabled to determine the confidence of the response.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_classifications = classify_completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: Display the document classifications\n",
    "print(document_classifications.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each document classification, group the page images together\n",
    "classified_images = []\n",
    "classified_images_pil = []\n",
    "\n",
    "for classification in document_classifications.classifications:\n",
    "    image_range_start = classification.image_range_start\n",
    "    image_range_end = classification.image_range_end\n",
    "\n",
    "    classified_images.append(page_images[image_range_start - 1:image_range_end])\n",
    "    classified_images_pil.append(pages[image_range_start - 1:image_range_end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, images in enumerate(classified_images):\n",
    "    print(f\"Classification {i}\")\n",
    "    \n",
    "    images_pil = classified_images_pil[i]\n",
    "    \n",
    "    pdf_bytes = io.BytesIO()\n",
    "    images_pil[0].save(pdf_bytes, format='PDF', resolution=100.0, save_all=True, append_images=images_pil[1:])\n",
    "    pdf_bytes.seek(0)\n",
    "\n",
    "    with Stopwatch() as di_stopwatch:\n",
    "        if use_document_intelligence:\n",
    "            poller = document_intelligence_client.begin_analyze_document(\n",
    "                model_id=\"prebuilt-layout\",\n",
    "                body=pdf_bytes,\n",
    "                output_content_format=DocumentContentFormat.MARKDOWN,\n",
    "                content_type=\"application/pdf\"\n",
    "            )\n",
    "            \n",
    "            result: AnalyzeResult = poller.result()\n",
    "\n",
    "            markdown = result.content\n",
    "            \n",
    "    # DEBUG: Display the document intelligence markdown content\n",
    "    display(Markdown(markdown))\n",
    "            \n",
    "    extract_system_prompt = f\"\"\"You are an AI assistant that extracts data from documents.\"\"\"\n",
    "\n",
    "    # Prepare the user content for the OpenAI API including any specific details for processing this type of document, text, and the document page images.\n",
    "    extract_user_content = []\n",
    "\n",
    "    extract_user_prompt = \"\"\"Extract the data from this invoice. \n",
    "    - If a value is not present, provide null.\n",
    "    - It is possible that there are multiple invoices in the same document across multiple pages.\n",
    "    - Some values must be inferred based on the content defined in the invoice.\n",
    "    - Dates should be in the format YYYY-MM-DD.\"\"\"\n",
    "\n",
    "    extract_user_content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": extract_user_prompt\n",
    "    })\n",
    "\n",
    "    if use_document_intelligence:\n",
    "        extract_user_content.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\": markdown\n",
    "        })\n",
    "        \n",
    "    extract_user_content.extend(images)\n",
    "\n",
    "    with Stopwatch() as oai_stopwatch:\n",
    "        completion = openai_client.beta.chat.completions.parse(\n",
    "            model=settings.gpt4o_model_deployment_name,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": extract_system_prompt,\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": extract_user_content\n",
    "                }\n",
    "            ],\n",
    "            response_format=Invoice,\n",
    "            max_tokens=4096,\n",
    "            temperature=0.1,\n",
    "            top_p=0.1,\n",
    "            logprobs=True # Enabled to determine the confidence of the response.\n",
    "        )\n",
    "        \n",
    "    # Gets the parsed Invoice object from the completion response.\n",
    "    invoice = completion.choices[0].message.parsed\n",
    "\n",
    "    expected_dict = expected_outputs[i].model_dump() if i < len(expected_outputs) else None\n",
    "    invoice_dict = invoice.model_dump()\n",
    "    \n",
    "    if expected_dict is not None:\n",
    "        accuracy = invoice_evaluator.evaluate(expected=expected_dict, actual=invoice_dict)\n",
    "    else:\n",
    "        accuracy = None\n",
    "\n",
    "    # Determines the confidence of the extracted data.\n",
    "    oai_confidence = evaluate_openai_confidence(invoice_dict, completion.choices[0])\n",
    "\n",
    "    if use_document_intelligence:\n",
    "        di_confidence = evaluate_di_confidence(invoice_dict, result)\n",
    "        confidence = merge_confidence_values(di_confidence, oai_confidence)\n",
    "    else:\n",
    "        confidence = oai_confidence\n",
    "        \n",
    "    # DEBUG: Gets the total execution time of the data extraction process.\n",
    "    total_elapsed = di_stopwatch.elapsed + image_stopwatch.elapsed + oai_stopwatch.elapsed\n",
    "\n",
    "    # DEBUG: Gets the prompt tokens and completion tokens from the completion response.\n",
    "    prompt_tokens = completion.usage.prompt_tokens\n",
    "    completion_tokens = completion.usage.completion_tokens\n",
    "\n",
    "    # DEBUG: Save the output of the data extraction result.\n",
    "    extraction_result = DataExtractionResult(invoice_dict, confidence, accuracy, prompt_tokens, completion_tokens, total_elapsed)\n",
    "    create_json_file(f\"{sample_path}/{sample_name}.{pdf_fname}_{i}.json\", extraction_result)\n",
    "    \n",
    "    display_accuracy = f\"{accuracy['overall'] * 100:.2f}%\" if accuracy is not None else \"N/A\"\n",
    "\n",
    "    # DEBUG: Display the outputs of the data extraction process.\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            \"Document\": pdf_fname,\n",
    "            \"Index\": i,\n",
    "            \"Accuracy\": display_accuracy,\n",
    "            \"Confidence\": f\"{confidence['_overall'] * 100:.2f}%\",\n",
    "            \"Execution Time\": f\"{total_elapsed:.2f} seconds\",\n",
    "            \"Document Intelligence Execution Time\": f\"{di_stopwatch.elapsed:.2f} seconds\",\n",
    "            \"Image Pre-processing Execution Time\": f\"{image_stopwatch.elapsed:.2f} seconds\",\n",
    "            \"OpenAI Execution Time\": f\"{oai_stopwatch.elapsed:.2f} seconds\",\n",
    "            \"Prompt Tokens\": prompt_tokens,\n",
    "            \"Completion Tokens\": completion_tokens\n",
    "        }\n",
    "    ])\n",
    "\n",
    "    display(df)\n",
    "    \n",
    "    if accuracy is not None:\n",
    "        display(get_extraction_comparison(expected_dict, invoice_dict, confidence, accuracy['accuracy']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
